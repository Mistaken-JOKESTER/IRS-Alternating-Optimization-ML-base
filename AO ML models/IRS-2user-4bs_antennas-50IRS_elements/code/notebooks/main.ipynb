{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers, initializers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ''\n",
    "\n",
    "hr_file = f\"D:/Ankit work/irs_2_users/data{dataset}/hr.csv\"\n",
    "hd_file = f\"D:/Ankit work/irs_2_users/data{dataset}/hd.csv\"\n",
    "G_file = f\"D:/Ankit work/irs_2_users/data{dataset}/G.csv\"\n",
    "Omega_file = f\"D:/Ankit work/irs_2_users/data{dataset}/omega.csv\"\n",
    "\n",
    "\n",
    "\n",
    "W_file = f\"D:/Ankit work/irs_2_users/data{dataset}/W.csv\"\n",
    "theta_file = f\"D:/Ankit work/irs_2_users/data{dataset}/theta.csv\"\n",
    "\n",
    "scaler_X_file = \"D:/Ankit work/irs_2_users/scaler_X.pkl\"\n",
    "scaler_Y_file = \"D:/Ankit work/irs_2_users/scaler_Y.pkl\"\n",
    "modal_save_file = \"D:/Ankit work/irs_2_users/my_trained_model.keras\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (555600, 620)\n",
      "Y shape:  (555600, 116)\n"
     ]
    }
   ],
   "source": [
    "# Load input datasets\n",
    "hr = pd.read_csv(hr_file, header=None)\n",
    "hd = pd.read_csv(hd_file, header=None)\n",
    "G = pd.read_csv(G_file, header=None)\n",
    "Omega = pd.read_csv(Omega_file, header=None)\n",
    "\n",
    "# Load output datasets\n",
    "W = pd.read_csv(W_file, header=None)\n",
    "Theta = pd.read_csv(theta_file, header=None)\n",
    "\n",
    "# Concatenate input datasets (hr, hd, G) and output datasets (W, theta)\n",
    "X = pd.concat([hr, hd, G, Omega], axis=1)\n",
    "Y = pd.concat([W, Theta], axis=1)\n",
    "\n",
    "# Standardize input data\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "# Optionally, scale output data (depending on your needs)\n",
    "scaler_Y = StandardScaler()\n",
    "Y_scaled = scaler_Y.fit_transform(Y)\n",
    "\n",
    "print(\"X shape: \", X.shape)\n",
    "print(\"Y shape: \", Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(333360, 620) (333360, 116)\n"
     ]
    }
   ],
   "source": [
    "# Manually shuffle the dataset using pandas\n",
    "X_shuffled = pd.DataFrame(X_scaled).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "Y_shuffled = pd.DataFrame(Y_scaled).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "X_train, X_temp, Y_train, Y_temp = train_test_split(X_scaled, Y_scaled, test_size=0.4, random_state=42)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_lambda = 1e-08\n",
    "dropout_rate = 0.001  # Set the dropout rate (0.2 = 20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, regularizers, initializers\n",
    "\n",
    "# Define the neural network\n",
    "model = models.Sequential()\n",
    "\n",
    "# Input layer (shape based on input features)\n",
    "model.add(layers.InputLayer(shape=(X_train.shape[1],)))\n",
    "\n",
    "# Hidden layers with Batch Normalization, Activation, and Dropout\n",
    "model.add(layers.Dense(1024, kernel_initializer=initializers.HeNormal(), kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dropout(dropout_rate)) \n",
    "\n",
    "model.add(layers.Dense(1024, kernel_initializer=initializers.HeNormal(), kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dropout(dropout_rate)) \n",
    "\n",
    "model.add(layers.Dense(512, kernel_initializer=initializers.HeNormal(), kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dropout(dropout_rate)) \n",
    "\n",
    "# Hidden layers with Batch Normalization, Activation, and Dropout\n",
    "model.add(layers.Dense(512, kernel_initializer=initializers.HeNormal(), kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dropout(dropout_rate)) \n",
    "\n",
    "model.add(layers.Dense(256, kernel_initializer=initializers.HeNormal(), kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dropout(dropout_rate))  # Dropout with dropout_rate probability\n",
    "\n",
    "model.add(layers.Dense(150, kernel_initializer=initializers.HeNormal(), kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dropout(dropout_rate))\n",
    "\n",
    "model.add(layers.Dense(128, kernel_initializer=initializers.HeNormal(), kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dropout(dropout_rate))\n",
    "\n",
    "model.add(layers.Dense(128, kernel_initializer=initializers.HeNormal(), kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dropout(dropout_rate))\n",
    "\n",
    "# Output layer (no activation function for regression)\n",
    "model.add(layers.Dense(Y_train.shape[1]))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "# Create the ReduceLROnPlateau callback\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 20ms/step - loss: 1.0002 - mae: 0.8793 - val_loss: 0.8912 - val_mae: 0.8240 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 20ms/step - loss: 0.8805 - mae: 0.8170 - val_loss: 0.8654 - val_mae: 0.8049 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 20ms/step - loss: 0.8590 - mae: 0.8018 - val_loss: 0.8335 - val_mae: 0.7830 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 20ms/step - loss: 0.8237 - mae: 0.7784 - val_loss: 0.7921 - val_mae: 0.7573 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 20ms/step - loss: 0.7712 - mae: 0.7436 - val_loss: 0.7203 - val_mae: 0.7054 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 20ms/step - loss: 0.7115 - mae: 0.7024 - val_loss: 0.6743 - val_mae: 0.6740 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 20ms/step - loss: 0.6666 - mae: 0.6713 - val_loss: 0.6424 - val_mae: 0.6524 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 20ms/step - loss: 0.6349 - mae: 0.6503 - val_loss: 0.6081 - val_mae: 0.6288 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 20ms/step - loss: 0.6028 - mae: 0.6286 - val_loss: 0.5766 - val_mae: 0.6071 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 20ms/step - loss: 0.5731 - mae: 0.6084 - val_loss: 0.5500 - val_mae: 0.5887 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 20ms/step - loss: 0.5467 - mae: 0.5902 - val_loss: 0.5284 - val_mae: 0.5718 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 20ms/step - loss: 0.5224 - mae: 0.5735 - val_loss: 0.5062 - val_mae: 0.5567 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 20ms/step - loss: 0.5022 - mae: 0.5596 - val_loss: 0.4927 - val_mae: 0.5475 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 20ms/step - loss: 0.4837 - mae: 0.5468 - val_loss: 0.4689 - val_mae: 0.5303 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 20ms/step - loss: 0.4635 - mae: 0.5330 - val_loss: 0.4504 - val_mae: 0.5169 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 20ms/step - loss: 0.4460 - mae: 0.5208 - val_loss: 0.4351 - val_mae: 0.5070 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 20ms/step - loss: 0.4402 - mae: 0.5168 - val_loss: 0.4204 - val_mae: 0.4958 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 20ms/step - loss: 0.4204 - mae: 0.5032 - val_loss: 0.4081 - val_mae: 0.4863 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 20ms/step - loss: 0.4062 - mae: 0.4928 - val_loss: 0.4102 - val_mae: 0.4885 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 20ms/step - loss: 0.3995 - mae: 0.4882 - val_loss: 0.3878 - val_mae: 0.4714 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 20ms/step - loss: 0.3860 - mae: 0.4787 - val_loss: 0.3824 - val_mae: 0.4680 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 20ms/step - loss: 0.3752 - mae: 0.4712 - val_loss: 0.3677 - val_mae: 0.4567 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 20ms/step - loss: 0.3649 - mae: 0.4638 - val_loss: 0.3599 - val_mae: 0.4509 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 20ms/step - loss: 0.3565 - mae: 0.4578 - val_loss: 0.3516 - val_mae: 0.4448 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 20ms/step - loss: 0.3492 - mae: 0.4526 - val_loss: 0.3426 - val_mae: 0.4374 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 20ms/step - loss: 0.3384 - mae: 0.4444 - val_loss: 0.3370 - val_mae: 0.4335 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 20ms/step - loss: 0.3322 - mae: 0.4396 - val_loss: 0.3317 - val_mae: 0.4287 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 21ms/step - loss: 0.3282 - mae: 0.4367 - val_loss: 0.3292 - val_mae: 0.4269 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 21ms/step - loss: 0.3258 - mae: 0.4349 - val_loss: 0.3310 - val_mae: 0.4279 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 21ms/step - loss: 0.3214 - mae: 0.4314 - val_loss: 0.3223 - val_mae: 0.4219 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 21ms/step - loss: 0.3241 - mae: 0.4336 - val_loss: 0.3175 - val_mae: 0.4169 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 21ms/step - loss: 0.3164 - mae: 0.4275 - val_loss: 0.3147 - val_mae: 0.4144 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 21ms/step - loss: 0.3093 - mae: 0.4218 - val_loss: 0.3131 - val_mae: 0.4133 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 21ms/step - loss: 0.3079 - mae: 0.4206 - val_loss: 0.3108 - val_mae: 0.4114 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 21ms/step - loss: 0.3072 - mae: 0.4201 - val_loss: 0.3055 - val_mae: 0.4069 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 21ms/step - loss: 0.2990 - mae: 0.4137 - val_loss: 0.3050 - val_mae: 0.4067 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 21ms/step - loss: 0.3028 - mae: 0.4166 - val_loss: 0.3041 - val_mae: 0.4058 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 21ms/step - loss: 0.2972 - mae: 0.4123 - val_loss: 0.2996 - val_mae: 0.4025 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 21ms/step - loss: 0.2934 - mae: 0.4094 - val_loss: 0.2993 - val_mae: 0.4018 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 21ms/step - loss: 0.2910 - mae: 0.4075 - val_loss: 0.2971 - val_mae: 0.4006 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 21ms/step - loss: 0.2934 - mae: 0.4093 - val_loss: 0.2939 - val_mae: 0.3975 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 21ms/step - loss: 0.2858 - mae: 0.4033 - val_loss: 0.2922 - val_mae: 0.3964 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 21ms/step - loss: 0.2851 - mae: 0.4028 - val_loss: 0.2933 - val_mae: 0.3971 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 21ms/step - loss: 0.2846 - mae: 0.4025 - val_loss: 0.2888 - val_mae: 0.3933 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 21ms/step - loss: 0.2838 - mae: 0.4021 - val_loss: 0.2907 - val_mae: 0.3948 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 21ms/step - loss: 0.2817 - mae: 0.4002 - val_loss: 0.2916 - val_mae: 0.3962 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 21ms/step - loss: 0.2795 - mae: 0.3984 - val_loss: 0.2849 - val_mae: 0.3901 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 21ms/step - loss: 0.2780 - mae: 0.3971 - val_loss: 0.2909 - val_mae: 0.3957 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 21ms/step - loss: 0.2805 - mae: 0.3992 - val_loss: 0.2841 - val_mae: 0.3899 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 21ms/step - loss: 0.2749 - mae: 0.3946 - val_loss: 0.2822 - val_mae: 0.3879 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 21ms/step - loss: 0.2742 - mae: 0.3939 - val_loss: 0.2806 - val_mae: 0.3867 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 21ms/step - loss: 0.2705 - mae: 0.3909 - val_loss: 0.2804 - val_mae: 0.3863 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 21ms/step - loss: 0.2695 - mae: 0.3901 - val_loss: 0.2780 - val_mae: 0.3843 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 21ms/step - loss: 0.2681 - mae: 0.3889 - val_loss: 0.2890 - val_mae: 0.3954 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 21ms/step - loss: 0.2716 - mae: 0.3919 - val_loss: 0.2744 - val_mae: 0.3814 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 21ms/step - loss: 0.2634 - mae: 0.3851 - val_loss: 0.2750 - val_mae: 0.3819 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 21ms/step - loss: 0.2637 - mae: 0.3853 - val_loss: 0.2734 - val_mae: 0.3802 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 21ms/step - loss: 0.2633 - mae: 0.3848 - val_loss: 0.2733 - val_mae: 0.3801 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 21ms/step - loss: 0.2607 - mae: 0.3828 - val_loss: 0.2699 - val_mae: 0.3770 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 21ms/step - loss: 0.2608 - mae: 0.3827 - val_loss: 0.2695 - val_mae: 0.3768 - learning_rate: 0.0010\n",
      "Epoch 61/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 21ms/step - loss: 0.2606 - mae: 0.3825 - val_loss: 0.2685 - val_mae: 0.3763 - learning_rate: 0.0010\n",
      "Epoch 62/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 21ms/step - loss: 0.2600 - mae: 0.3822 - val_loss: 0.2688 - val_mae: 0.3763 - learning_rate: 0.0010\n",
      "Epoch 63/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 21ms/step - loss: 0.2580 - mae: 0.3805 - val_loss: 0.2680 - val_mae: 0.3751 - learning_rate: 0.0010\n",
      "Epoch 64/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 21ms/step - loss: 0.2576 - mae: 0.3801 - val_loss: 0.2680 - val_mae: 0.3754 - learning_rate: 0.0010\n",
      "Epoch 65/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 21ms/step - loss: 0.2565 - mae: 0.3791 - val_loss: 0.2662 - val_mae: 0.3740 - learning_rate: 0.0010\n",
      "Epoch 66/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 21ms/step - loss: 0.2636 - mae: 0.3852 - val_loss: 0.2650 - val_mae: 0.3726 - learning_rate: 0.0010\n",
      "Epoch 67/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 21ms/step - loss: 0.2570 - mae: 0.3796 - val_loss: 0.2652 - val_mae: 0.3729 - learning_rate: 0.0010\n",
      "Epoch 68/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 21ms/step - loss: 0.2543 - mae: 0.3770 - val_loss: 0.2649 - val_mae: 0.3726 - learning_rate: 0.0010\n",
      "Epoch 69/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 21ms/step - loss: 0.2526 - mae: 0.3757 - val_loss: 0.2646 - val_mae: 0.3724 - learning_rate: 0.0010\n",
      "Epoch 70/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 21ms/step - loss: 0.2516 - mae: 0.3747 - val_loss: 0.2634 - val_mae: 0.3711 - learning_rate: 0.0010\n",
      "Epoch 71/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 22ms/step - loss: 0.2561 - mae: 0.3786 - val_loss: 0.2621 - val_mae: 0.3701 - learning_rate: 0.0010\n",
      "Epoch 72/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 22ms/step - loss: 0.2532 - mae: 0.3761 - val_loss: 0.2619 - val_mae: 0.3696 - learning_rate: 0.0010\n",
      "Epoch 73/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 21ms/step - loss: 0.2488 - mae: 0.3723 - val_loss: 0.2623 - val_mae: 0.3700 - learning_rate: 0.0010\n",
      "Epoch 74/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 21ms/step - loss: 0.2586 - mae: 0.3807 - val_loss: 0.2594 - val_mae: 0.3673 - learning_rate: 0.0010\n",
      "Epoch 75/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 22ms/step - loss: 0.2480 - mae: 0.3714 - val_loss: 0.2683 - val_mae: 0.3762 - learning_rate: 0.0010\n",
      "Epoch 76/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 22ms/step - loss: 0.2593 - mae: 0.3811 - val_loss: 0.2588 - val_mae: 0.3672 - learning_rate: 0.0010\n",
      "Epoch 77/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 22ms/step - loss: 0.2476 - mae: 0.3712 - val_loss: 0.2592 - val_mae: 0.3673 - learning_rate: 0.0010\n",
      "Epoch 78/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 22ms/step - loss: 0.2484 - mae: 0.3718 - val_loss: 0.2663 - val_mae: 0.3746 - learning_rate: 0.0010\n",
      "Epoch 79/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 22ms/step - loss: 0.2489 - mae: 0.3724 - val_loss: 0.2596 - val_mae: 0.3676 - learning_rate: 0.0010\n",
      "Epoch 80/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 22ms/step - loss: 0.2459 - mae: 0.3697 - val_loss: 0.2709 - val_mae: 0.3785 - learning_rate: 0.0010\n",
      "Epoch 81/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 22ms/step - loss: 0.2496 - mae: 0.3728 - val_loss: 0.2565 - val_mae: 0.3648 - learning_rate: 0.0010\n",
      "Epoch 82/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 22ms/step - loss: 0.2454 - mae: 0.3692 - val_loss: 0.2598 - val_mae: 0.3680 - learning_rate: 0.0010\n",
      "Epoch 83/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 22ms/step - loss: 0.2459 - mae: 0.3697 - val_loss: 0.2566 - val_mae: 0.3653 - learning_rate: 0.0010\n",
      "Epoch 84/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 22ms/step - loss: 0.2447 - mae: 0.3686 - val_loss: 0.2573 - val_mae: 0.3655 - learning_rate: 0.0010\n",
      "Epoch 85/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 22ms/step - loss: 0.2495 - mae: 0.3727 - val_loss: 0.2545 - val_mae: 0.3630 - learning_rate: 0.0010\n",
      "Epoch 86/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 22ms/step - loss: 0.2438 - mae: 0.3678 - val_loss: 0.2567 - val_mae: 0.3651 - learning_rate: 0.0010\n",
      "Epoch 87/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 22ms/step - loss: 0.2478 - mae: 0.3714 - val_loss: 0.2541 - val_mae: 0.3630 - learning_rate: 0.0010\n",
      "Epoch 88/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 22ms/step - loss: 0.2415 - mae: 0.3660 - val_loss: 0.2540 - val_mae: 0.3630 - learning_rate: 0.0010\n",
      "Epoch 89/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 22ms/step - loss: 0.2437 - mae: 0.3678 - val_loss: 0.2542 - val_mae: 0.3633 - learning_rate: 0.0010\n",
      "Epoch 90/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 22ms/step - loss: 0.2426 - mae: 0.3668 - val_loss: 0.2544 - val_mae: 0.3634 - learning_rate: 0.0010\n",
      "Epoch 91/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 22ms/step - loss: 0.2416 - mae: 0.3659 - val_loss: 0.2536 - val_mae: 0.3626 - learning_rate: 0.0010\n",
      "Epoch 92/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 22ms/step - loss: 0.2417 - mae: 0.3660 - val_loss: 0.2551 - val_mae: 0.3639 - learning_rate: 0.0010\n",
      "Epoch 93/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 22ms/step - loss: 0.2424 - mae: 0.3666 - val_loss: 0.2526 - val_mae: 0.3618 - learning_rate: 0.0010\n",
      "Epoch 94/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 22ms/step - loss: 0.2387 - mae: 0.3635 - val_loss: 0.2534 - val_mae: 0.3626 - learning_rate: 0.0010\n",
      "Epoch 95/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 22ms/step - loss: 0.2392 - mae: 0.3639 - val_loss: 0.2516 - val_mae: 0.3611 - learning_rate: 0.0010\n",
      "Epoch 96/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 22ms/step - loss: 0.2392 - mae: 0.3638 - val_loss: 0.2515 - val_mae: 0.3604 - learning_rate: 0.0010\n",
      "Epoch 97/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 22ms/step - loss: 0.2388 - mae: 0.3634 - val_loss: 0.2497 - val_mae: 0.3588 - learning_rate: 0.0010\n",
      "Epoch 98/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 23ms/step - loss: 0.2381 - mae: 0.3630 - val_loss: 0.2504 - val_mae: 0.3597 - learning_rate: 0.0010\n",
      "Epoch 99/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 23ms/step - loss: 0.2366 - mae: 0.3617 - val_loss: 0.2512 - val_mae: 0.3605 - learning_rate: 0.0010\n",
      "Epoch 100/100\n",
      "\u001b[1m2605/2605\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 23ms/step - loss: 0.2358 - mae: 0.3610 - val_loss: 0.2483 - val_mae: 0.3577 - learning_rate: 0.0010\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, epochs=100, validation_data=(X_val, Y_val), batch_size=128, shuffle=True, callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10418/10418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3ms/step - loss: 0.1958 - mae: 0.3200\n",
      "\u001b[1m3473/3473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.2501 - mae: 0.3588\n",
      "\u001b[1m3473/3473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.2479 - mae: 0.3577\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model on training, validation, and test sets\n",
    "train_loss, train_mae = model.evaluate(X_train, Y_train)\n",
    "val_loss, val_mae = model.evaluate(X_val, Y_val)\n",
    "test_loss, test_mae = model.evaluate(X_test, Y_test)\n",
    "\n",
    "# Get model predictions for calculating MAPE\n",
    "# train_pred = model.predict(X_train)\n",
    "# val_pred = model.predict(X_val)\n",
    "# test_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "# Create a remark about model performance (e.g., overfitting or good fit)\n",
    "if train_loss < val_loss and train_loss < test_loss:\n",
    "    remark = \"Possible overfitting detected.\"\n",
    "elif val_loss <= test_loss:\n",
    "    remark = \"Good fit.\"\n",
    "else:\n",
    "    remark = \"Model underfits the data.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you already have your model trained, and 'history' stores training info\n",
    "# Redirect model summary to a file\n",
    "with open('D:/Ankit work/irs_2_users/model_performance_log.txt', \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write('\\n*******************************************************************************************************\\n')\n",
    "    f.write(f\"Remark: {remark}\\n\")\n",
    "    f.write(f\"He intialized , L2 Regularized ({l2_lambda}), dropout({dropout_rate}), early stop (20), batch normalization\\n\" )\n",
    "    # Save the number of hidden layers and neurons per layer\n",
    "    # model.summary(print_fn=lambda x: f.write(x + \"\\n\"))\n",
    "    \n",
    "    # Save the number of hidden layers and neurons per layer\n",
    "    hidden_layers = [layer for layer in model.layers if isinstance(layer, tf.keras.layers.Dense) and layer != model.layers[-1]]  # exclude output layer\n",
    "    f.write(\"\\nNumber of hidden layers: {}\\n\".format(len(hidden_layers)))\n",
    "    for i, layer in enumerate(hidden_layers):\n",
    "        f.write(\"Layer {}: {} neurons\\n\".format(i + 1, layer.units))  # 'units' gives the number of neurons\n",
    "\n",
    "    f.write(\"\\nTraining loss: {}\\n\".format(train_loss))\n",
    "    f.write(\"Validation loss: {}\\n\".format(val_loss))\n",
    "    f.write(\"Test loss: {}\\n\".format(test_loss))\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"\\nTraining and Validation Losses for each epoch:\\n\")\n",
    "    for epoch in range(len(history.history['loss'])):\n",
    "        if(epoch % 10 == 0):\n",
    "            f.write(\"Epoch {}: Training loss = {:.4f}, Validation loss = {:.4f}\\n\".format(\n",
    "                epoch + 1,\n",
    "                history.history['loss'][epoch],\n",
    "                history.history['val_loss'][epoch]\n",
    "            ))\n",
    "            \n",
    "    f.write('*******************************************************************************************************\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:/Ankit work/irs_2_users/scaler_Y.pkl']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained model to a file (HDF5 format or TensorFlow SavedModel format)\n",
    "model.save(modal_save_file)  # Save as HDF5\n",
    "\n",
    "# Save the scaler to a file\n",
    "joblib.dump(scaler_X, scaler_X_file)\n",
    "joblib.dump(scaler_Y, scaler_Y_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load the model\n",
    "loaded_model = keras.models.load_model(modal_save_file)\n",
    "\n",
    "# Load the saved scaler\n",
    "scaler_X_loaded = joblib.load(scaler_X_file)\n",
    "scaler_Y_loaded = joblib.load(scaler_Y_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_model = model\n",
    "scaler_X_loaded = scaler_X\n",
    "scaler_Y_loaded = scaler_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">635,904</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_41          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_42          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_43          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_44          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_45          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_51 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">38,550</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_46          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_52 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">19,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_47          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_53 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_48          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_54 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">116</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">14,964</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_46 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │       \u001b[38;5;34m635,904\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_41          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │         \u001b[38;5;34m4,096\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_41 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_41 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_47 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │     \u001b[38;5;34m1,049,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_42          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │         \u001b[38;5;34m4,096\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_42 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_42 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m524,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_43          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_43 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_43 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m262,656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_44          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_44 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_44 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_50 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_45          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_45 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_45 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_51 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)            │        \u001b[38;5;34m38,550\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_46          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)            │           \u001b[38;5;34m600\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_46 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_46 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_52 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m19,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_47          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_47 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_47 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_53 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_48          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_48 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_48 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_54 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m116\u001b[0m)            │        \u001b[38;5;34m14,964\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,110,800</span> (30.94 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,110,800\u001b[0m (30.94 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,701,110</span> (10.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,701,110\u001b[0m (10.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,468</span> (29.17 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m7,468\u001b[0m (29.17 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,402,222</span> (20.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m5,402,222\u001b[0m (20.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 620)\n"
     ]
    }
   ],
   "source": [
    "hr_testing_file = f\"D:/Ankit work/irs_2_users/testing_data/hr.csv\"\n",
    "hd_testing_file = f\"D:/Ankit work/irs_2_users/testing_data/hd.csv\"\n",
    "G_testing_file = f\"D:/Ankit work/irs_2_users/testing_data/G.csv\"\n",
    "Omega_testing_file = f\"D:/Ankit work/irs_2_users/testing_data/omega.csv\"\n",
    "Theta_testing_file = f\"D:/Ankit work/irs_2_users/testing_data/theta.csv\"\n",
    "\n",
    "# Load input datasets\n",
    "hr_testing = pd.read_csv(hr_testing_file, header=None)\n",
    "hd_testing = pd.read_csv(hd_testing_file, header=None)\n",
    "G_testing = pd.read_csv(G_testing_file, header=None)\n",
    "Omega_testing = pd.read_csv(Omega_testing_file, header=None)\n",
    "\n",
    "# Concatenate input datasets (hr, hd, G) and output datasets (W, theta)\n",
    "X_testing = pd.concat([hr_testing, hd_testing, G_testing, Omega_testing], axis=1)\n",
    "\n",
    "X_scaled_testing = scaler_X_loaded.transform(X_testing)\n",
    "print(X_testing.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Predictions for the batch of inputs: (100, 116)\n",
      "\u001b[1m3473/3473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 0.2479 - mae: 0.3577\n",
      "Test Loss: 0.24813798069953918\n",
      "Test MAE: 0.3576447367668152\n"
     ]
    }
   ],
   "source": [
    "# Let's assume 'new_data_batch' is a batch of new inputs (e.g., multiple samples).\n",
    "new_data_batch = X_scaled_testing\n",
    "\n",
    "# Make predictions for the batch\n",
    "scaled_predictions = loaded_model.predict(new_data_batch)\n",
    "predictions = scaler_Y_loaded.inverse_transform(scaled_predictions)\n",
    "\n",
    "# Print the predictions\n",
    "print(\"Predictions for the batch of inputs:\", predictions.shape)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_mae = loaded_model.evaluate(X_test, Y_test)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test MAE: {test_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming Y_pred is a NumPy array\n",
    "# If it's not, convert it to a NumPy array: Y_pred = np.array(Y_pred)\n",
    "\n",
    "# Convert to DataFrame\n",
    "Y_pred_df = pd.DataFrame(predictions)  # Add appropriate column name(s)\n",
    "\n",
    "# Save to CSV\n",
    "Y_pred_df.to_csv('D:/Ankit work/irs_2_users/testing_data/common_pred.csv', index=False, header=None)  # index=False to avoid adding row numbers to the file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
